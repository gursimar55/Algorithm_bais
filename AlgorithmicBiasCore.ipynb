{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Bias \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "bcDB = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0.2776</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.22</td>\n",
       "      <td>23.12</td>\n",
       "      <td>94.37</td>\n",
       "      <td>609.9</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.1981</td>\n",
       "      <td>0.06618</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>0.07542</td>\n",
       "      <td>...</td>\n",
       "      <td>37.18</td>\n",
       "      <td>106.4</td>\n",
       "      <td>762.4</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.9327</td>\n",
       "      <td>0.8488</td>\n",
       "      <td>0.1772</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.14460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.34</td>\n",
       "      <td>26.86</td>\n",
       "      <td>81.15</td>\n",
       "      <td>477.4</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.1353</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.04562</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.06937</td>\n",
       "      <td>...</td>\n",
       "      <td>39.34</td>\n",
       "      <td>101.7</td>\n",
       "      <td>768.9</td>\n",
       "      <td>0.1785</td>\n",
       "      <td>0.4706</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.12050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.86</td>\n",
       "      <td>23.21</td>\n",
       "      <td>100.40</td>\n",
       "      <td>671.4</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.1697</td>\n",
       "      <td>0.08878</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.06672</td>\n",
       "      <td>...</td>\n",
       "      <td>27.78</td>\n",
       "      <td>118.6</td>\n",
       "      <td>784.7</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.4648</td>\n",
       "      <td>0.4589</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.08701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.77</td>\n",
       "      <td>22.29</td>\n",
       "      <td>90.63</td>\n",
       "      <td>588.9</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1267</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.06526</td>\n",
       "      <td>0.1834</td>\n",
       "      <td>0.06877</td>\n",
       "      <td>...</td>\n",
       "      <td>34.01</td>\n",
       "      <td>111.6</td>\n",
       "      <td>806.9</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>0.3809</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>0.09333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0           0.1184   \n",
       "1        14.22         23.12           94.37      609.9           0.1075   \n",
       "2        12.34         26.86           81.15      477.4           0.1034   \n",
       "3        14.86         23.21          100.40      671.4           0.1044   \n",
       "4        13.77         22.29           90.63      588.9           0.1200   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.2776          0.3001              0.14710         0.2419   \n",
       "1            0.2413          0.1981              0.06618         0.2384   \n",
       "2            0.1353          0.1085              0.04562         0.1943   \n",
       "3            0.1980          0.1697              0.08878         0.1737   \n",
       "4            0.1267          0.1385              0.06526         0.1834   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33            184.6      2019.0   \n",
       "1                 0.07542  ...          37.18            106.4       762.4   \n",
       "2                 0.06937  ...          39.34            101.7       768.9   \n",
       "3                 0.06672  ...          27.78            118.6       784.7   \n",
       "4                 0.06877  ...          34.01            111.6       806.9   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1533             0.9327           0.8488                0.1772   \n",
       "2            0.1785             0.4706           0.4425                0.1459   \n",
       "3            0.1316             0.4648           0.4589                0.1727   \n",
       "4            0.1737             0.3122           0.3809                0.1673   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.5166                  0.14460       0  \n",
       "2          0.3215                  0.12050       0  \n",
       "3          0.3000                  0.08701       0  \n",
       "4          0.3080                  0.09333       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcDF = pd.DataFrame(bcDB.data, columns= list(bcDB['feature_names']))\n",
    "bcDF['target'] = pd.Series(bcDB.target)\n",
    "bcDF = bcDF.sort_values(by = ['target'])\n",
    "bcDF = bcDF.reset_index(drop=True)\n",
    "bcDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 malignant\n",
      "357 benign\n"
     ]
    }
   ],
   "source": [
    "vc = bcDF['target'].value_counts()\n",
    "for i,j in enumerate(bcDB.target_names):\n",
    "    print (vc[i],j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = bcDF.pop('target').values\n",
    "X = bcDF.values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-NN\n",
    "Malignant is the minority class at ~40%. k-NN classifier picks up this under-representation and accentuates it, predicting just 36% malignant. \n",
    "k-NN classifier is bais towards the majority class as the predicted score is 36%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.36\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "kNN = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = kNN.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "Malignant is the minority class at ~40%. Decision Tree Classifier picks up this under-representation and accentuates it, predicting just 38% malignant. Decision Tree Classifier is also slighly bias towards the majority class as the predicted score is 38%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DT = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = DT.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Malignant is the minority class at ~40%.  \n",
    "Logistic Regression Classifier picks up this under-representation and accentuates it, predicting just 37% malignant. \n",
    "Logistic Regression Classifier is also bias towards the majority class as the predicted score is 37%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = LR.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes\n",
    "Naive Bayes is probability based learning. \n",
    "Malignant is the minority class at ~40%.  \n",
    "Gaussian Naive Bayes picks up this under-representation and accentuates it, predicting just 39% malignant. \n",
    "Gaussian Naive Bayes is very slighly bias towards the majority class as the predicted score is 39%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "m3 = GaussianNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = m3.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes\n",
    "Malignant is the minority class at ~40%.  \n",
    "Multinomial Naive Bayes picks up this under-representation and accentuates it, predicting just 30% malignant.\n",
    "Multinomail Naive Bayes is highly bias towards the majority class as the predicted score is 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "m1 = MultinomialNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = m1.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes\n",
    "Malignant is the minority class at ~40%.  \n",
    "Complement Naive Bayes picks up this under-representation and accentuates it, predicting just 31% malignant.\n",
    "Complement Naive Bayes is highly bias towards the majority class as the predicted score is 31%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (143, 30)\n",
      "Malignant in test set : 0.40\n",
      "Predicted malignant : 0.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "m2 = ComplementNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "y_pred = m2.fit(X_train, y_train).predict(X_test)\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "y_test.sum()/len(y_test)\n",
    "\n",
    "print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Out Testing\n",
    "The dataset is divided into train set and test set. In this, I have set the test_size to 0.2. So, 80% data is for training and 20% of data is for testing. Test data is used to see how well does the model performs. The accuracy score is calculated for test set. \n",
    "Logestic Regression has heighest accuracy of 94%. Multinomail and complement naive bayes has lowest accuracy of 87%.\n",
    "Accuracy score is not enough to justify the bias, as the predicted value might be because of the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score\n",
      "The accuracy score on Test set KNeighborsClassifier   0.91\n",
      "The accuracy score on Test set DecisionTreeClassifier 0.92\n",
      "The accuracy score on Test set LogisticRegression     0.94\n",
      "The accuracy score on Test set GaussianNB             0.88\n",
      "The accuracy score on Test set MultinomialNB          0.87\n",
      "The accuracy score on Test set ComplementNB           0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = [kNN,DT,LR,m3,m1,m2]\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=2, test_size=0.2)\n",
    "print('Accuracy Score')\n",
    "for m in models:\n",
    "    mm = m.fit(X_train1, y_train1)\n",
    "    y_pred1 = mm.predict(X_test1)\n",
    "    a = accuracy_score(y_test1, y_pred1) \n",
    "    print(\"The accuracy score on Test set {:22} {:.2f}\".format(type(m).__name__, a)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation \n",
    "In cross validation the data set is split into k folds and the each fold or group is used as the test set and the rest as training set. Here, the folds is set to 4, so this is 4-folds corss validation. Logistic Regression has the lowest false positive, 8% and true positive of 97%. \n",
    "\n",
    "The TN rate of k-NN classifier is 87% and it's TP rate is 96%, so this classifier is baised towards the majority class.\n",
    "\n",
    "The TN rate of  Decision Tree Classfier is 90% and it's TP rate is 94%, so the classifier is not much baised towards the majority class.\n",
    "\n",
    "The TN rate of Logistic Regression Classifier is 92% and it's TP rate is 97%, so this classifier is slightly baised towards the majority class.\n",
    "\n",
    "The TN rate of Gaussian Naive Bayes Classifier is 89% and it's TP rate is 97%, so this classifier is highly baised towards the majority class.\n",
    "\n",
    "The TN rate of Multinomial Naive Bayes Classifier is 77% and it's TP rate is 97%, so this classifier is the most baised towards the majority class.\n",
    "\n",
    "The TN rate of Complement Naive Bayes Classifier is 77% and it's TP rate is 97%, so this classifier is the most baised towards the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
    "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    "\n",
    "models = [kNN,DT,LR,m3,m1,m2]\n",
    "\n",
    "folds = 4\n",
    "v = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 x CV KNeighborsClassifier   FP: 0.13  TP: 0.96 TN: 0.87\n",
      "4 x CV DecisionTreeClassifier FP: 0.09  TP: 0.94 TN: 0.91\n",
      "4 x CV LogisticRegression     FP: 0.08  TP: 0.97 TN: 0.92\n",
      "4 x CV GaussianNB             FP: 0.11  TP: 0.97 TN: 0.89\n",
      "4 x CV MultinomialNB          FP: 0.23  TP: 0.97 TN: 0.77\n",
      "4 x CV ComplementNB           FP: 0.23  TP: 0.97 TN: 0.77\n"
     ]
    }
   ],
   "source": [
    "TNP = []\n",
    "for m in models:\n",
    "    cv_results = cross_validate(m, X, y, cv= folds,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = v, n_jobs = -1)\n",
    "    fp_rate = cv_results['test_fp'].sum()/(cv_results['test_fp'].sum()+cv_results['test_tn'].sum())\n",
    "    tp_rate = cv_results['test_tp'].sum()/(cv_results['test_tp'].sum()+cv_results['test_fn'].sum())\n",
    "    tn_rate = cv_results['test_tn'].sum()/(cv_results['test_tn'].sum()+cv_results['test_fp'].sum())\n",
    "    TNP.append(round(tn_rate,2))\n",
    "  \n",
    "    print(\"{} x CV {:22} FP: {:.2f}  TP: {:.2f} TN: {:.2f}\".format(folds, type(m).__name__, fp_rate, tp_rate, tn_rate)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Predicted Score Plot\n",
    "The graph is plotted below. \n",
    "\n",
    "It can be observed that Logestic Regression has the highest TN rate, 92%. This classifier is slightly baised towards the majority class.\n",
    "\n",
    "Decision Tree Classifier has the TN rate of 90%. This classifer is also slightly baised towards the majority class.\n",
    "\n",
    "k-NN classifier has the TN rate of 87%. This classifer is baised towards the majority class.\n",
    "\n",
    "Gaussian Naive Bayes Classifier has the TN rate of 89%. This classifer is highly baised towards the majority class.\n",
    "\n",
    "Multinomial and Complement Naive Bayes Classifier has the TN rate of 77%. These classifers are the most baised towards the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87, 0.91, 0.92, 0.89, 0.77, 0.77]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "Yplot = TNP\n",
    "print(Yplot)\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('TN SCores')\n",
    "plt.bar(['KNN', 'DT', 'LR', 'GNB', 'MNB', 'CNB'], Yplot, 0.5)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sampling\n",
    "Random over sampler is used to overcome the bias. This generates artificial data and overcomes the bias. The random state is set to 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714, 30) (714,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=2)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "print(X_resampled.shape, y_resampled.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The malignant is now balanced class at 50%. \n",
    "\n",
    "Oversampling has balanced the data. \n",
    "\n",
    "The predicted Maligant using k-NN classifier is 45%, so it has almost overcome the bias.\n",
    "\n",
    "\n",
    "The predicted Maligant using Decision Tree classifier is 51%, it has overcomed the bias, but now it is based towards the minority class.\n",
    "\n",
    "\n",
    "The predicted Maligant using Logistic Regression classifier is 46%, so it has almost overcome the bias.\n",
    "\n",
    "\n",
    "The predicted Maligant using Gaussian Naive Bayes classifier is 44%, which shows the classifier is still biased.\n",
    "\n",
    "\n",
    "The predicted Maligant using Multinomial Naive Bayes Classifier is 37%, Which shows that the classifier is biased. \n",
    "\n",
    "\n",
    "The predicted Maligant using Complement Naive Bayes classifier is 37%, Which shows that the classifier is biased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.45\n",
      "DecisionTreeClassifier\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.50\n",
      "LogisticRegression\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.46\n",
      "GaussianNB\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.44\n",
      "MultinomialNB\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.37\n",
      "ComplementNB\n",
      "(535, 30) (179, 30)\n",
      "Malignant in test set : 0.50\n",
      "Predicted malignant : 0.37\n"
     ]
    }
   ],
   "source": [
    "models = [kNN,DT,LR,m3,m1,m2]\n",
    "for m in models:\n",
    "    print(type(m).__name__)\n",
    "\n",
    "    y_pred = m.fit(X_train, y_train).predict(X_test)\n",
    "    print(X_train.shape,X_test.shape)\n",
    "    y_test.sum()/len(y_test)\n",
    "    print(\"Malignant in test set : %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "    print(\"Predicted malignant : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Out Testing\n",
    "The overall accuracy of all the classifiers improved. \n",
    "\n",
    "Logistic regression Classifier has 95% accuracy. This classifier has the highest accuracy.\n",
    "\n",
    "Decision tree Classifier has 94% accuracy.\n",
    "\n",
    "Gaussian Naive Bayes Classifier has 93% accuracy.\n",
    "\n",
    "k-NN Classifier has 92% accuracy.\n",
    "\n",
    "\n",
    "Multinomial and Complement Naive Bayes Classifier has 85% accuracy. These classifiers have the lowest accuracy. \n",
    "\n",
    "The accuracy scores of all classifiers improved after oversampling. Oversampling produced a balanced dataset. Multinomial and Complement Naive Bayes Classifier are still biased after oversampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraqcy Score\n",
      "Accuracy Score on Test set KNeighborsClassifier   0.92\n",
      "Accuracy Score on Test set DecisionTreeClassifier 0.94\n",
      "Accuracy Score on Test set LogisticRegression     0.95\n",
      "Accuracy Score on Test set GaussianNB             0.93\n",
      "Accuracy Score on Test set MultinomialNB          0.85\n",
      "Accuracy Score on Test set ComplementNB           0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = [kNN,DT,LR,m3,m1,m2]\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=0, test_size=1/3)\n",
    "print('Accuraqcy Score')\n",
    "for m in models:\n",
    "    mm = m.fit(X_train, y_train)\n",
    "    y_pred = mm.predict(X_test)\n",
    "    f1s = accuracy_score(y_test, y_pred) \n",
    "    print(\"Accuracy Score on Test set {:22} {:.2f}\".format(type(m).__name__, f1s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The minority class in the dataset is reviewNotHelpfulness, at 38%.\n",
    "\n",
    "Predicted score for the minority class using k-NN classifier is 30%. The classifier is biased towards the majority class.\n",
    "\n",
    "Predicted score for the minority class using Decision Tress classifier is 27%. The classifier is highly biased towards the majority class.\n",
    "\n",
    "Predicted score for the minority class using Logistic Regression classifier is 21%. The classifier is highy biased towards the majority class.\n",
    "\n",
    "Predicted score for the minority class using Gaussian Naive Bayes classifier is 35%. The classifier is slightly biased towards the majority class.\n",
    "\n",
    "Logistic Regression Classifier is most biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aveHelpfulnessRatioUser</th>\n",
       "      <th>stdevHelpfulnessRatioUser</th>\n",
       "      <th>pcReviewsExceedMinHelpfulnessSupport</th>\n",
       "      <th>numReviewsUser</th>\n",
       "      <th>numReviewsHotel</th>\n",
       "      <th>ratingUser</th>\n",
       "      <th>numberSubRatingsUser</th>\n",
       "      <th>subRatingMeanUser</th>\n",
       "      <th>subRatingStdevUser</th>\n",
       "      <th>aveRatingUser</th>\n",
       "      <th>...</th>\n",
       "      <th>completeness_1</th>\n",
       "      <th>completeness_2</th>\n",
       "      <th>completeness_3</th>\n",
       "      <th>numberTermsEntry</th>\n",
       "      <th>percentageAlphaCharsEntry</th>\n",
       "      <th>fractionUpperCaseCharsEntry</th>\n",
       "      <th>fractionYouVsIEntry</th>\n",
       "      <th>numberTermsSummaryQuote</th>\n",
       "      <th>percentageAlphaCharsSummaryQuote</th>\n",
       "      <th>fractionUpperCaseCharsSummaryQuote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>0.788474</td>\n",
       "      <td>0.025703</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.096774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.772487</td>\n",
       "      <td>0.377321</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>233</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0.791888</td>\n",
       "      <td>0.012594</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.715473</td>\n",
       "      <td>0.300437</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>12</td>\n",
       "      <td>302</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0.799639</td>\n",
       "      <td>0.024831</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.521250</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.527778</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>0.782212</td>\n",
       "      <td>0.029155</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.246926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0.805128</td>\n",
       "      <td>0.028662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aveHelpfulnessRatioUser  stdevHelpfulnessRatioUser  \\\n",
       "0                 1.000000                   0.000000   \n",
       "1                 0.772487                   0.377321   \n",
       "2                 0.715473                   0.300437   \n",
       "3                 0.521250                   0.481675   \n",
       "4                 0.603175                   0.246926   \n",
       "\n",
       "   pcReviewsExceedMinHelpfulnessSupport  numReviewsUser  numReviewsHotel  \\\n",
       "0                              0.666667               3               16   \n",
       "1                              0.500000              12              233   \n",
       "2                              0.833333              12              302   \n",
       "3                              0.222222              36                6   \n",
       "4                              1.000000               2              271   \n",
       "\n",
       "   ratingUser  numberSubRatingsUser  subRatingMeanUser  subRatingStdevUser  \\\n",
       "0           5                     4           4.000000            0.000000   \n",
       "1           5                     0           0.000000            0.000000   \n",
       "2           4                     7           3.714286            0.755929   \n",
       "3           1                     4           1.000000            0.000000   \n",
       "4           3                     0           0.000000            0.000000   \n",
       "\n",
       "   aveRatingUser  ...  completeness_1  completeness_2  completeness_3  \\\n",
       "0       4.333333  ...               2               0               1   \n",
       "1       4.333333  ...               2               0               0   \n",
       "2       4.166667  ...               5               0               3   \n",
       "3       3.527778  ...               2               0               0   \n",
       "4       3.500000  ...               0               0               0   \n",
       "\n",
       "   numberTermsEntry  percentageAlphaCharsEntry  fractionUpperCaseCharsEntry  \\\n",
       "0               182                   0.788474                     0.025703   \n",
       "1               158                   0.791888                     0.012594   \n",
       "2                59                   0.799639                     0.024831   \n",
       "3                95                   0.782212                     0.029155   \n",
       "4                43                   0.805128                     0.028662   \n",
       "\n",
       "   fractionYouVsIEntry  numberTermsSummaryQuote  \\\n",
       "0             0.500000                        6   \n",
       "1             0.500000                        1   \n",
       "2             0.333333                        4   \n",
       "3             0.500000                        2   \n",
       "4             0.000000                        1   \n",
       "\n",
       "   percentageAlphaCharsSummaryQuote  fractionUpperCaseCharsSummaryQuote  \n",
       "0                          0.815789                            0.096774  \n",
       "1                          1.000000                            0.083333  \n",
       "2                          0.828571                            0.034483  \n",
       "3                          0.800000                            0.062500  \n",
       "4                          1.000000                            0.142857  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel = pd.read_csv('HotelRevHelpfulnessV2.csv')\n",
    "y = hotel.pop('reviewHelpfulness').values\n",
    "X = hotel.values\n",
    "hotel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [kNN,DT,LR,m3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "(364, 23) (122, 23)\n",
      "Not Helpfulness: 0.38\n",
      "Predicted Not Helpfulness : 0.30\n",
      "DecisionTreeClassifier\n",
      "(364, 23) (122, 23)\n",
      "Not Helpfulness: 0.38\n",
      "Predicted Not Helpfulness : 0.23\n",
      "LogisticRegression\n",
      "(364, 23) (122, 23)\n",
      "Not Helpfulness: 0.38\n",
      "Predicted Not Helpfulness : 0.21\n",
      "GaussianNB\n",
      "(364, 23) (122, 23)\n",
      "Not Helpfulness: 0.38\n",
      "Predicted Not Helpfulness : 0.35\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "    y_pred = m.fit(X_train, y_train).predict(X_test)\n",
    "    print(type(m).__name__)\n",
    "\n",
    "    print(X_train.shape,X_test.shape)\n",
    "\n",
    "    y_test.sum()/len(y_test)\n",
    "\n",
    "    print(\"Not Helpfulness: %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "    print(\"Predicted Not Helpfulness : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Out Testing\n",
    "The dataset is divided into train set and test set. In this, I have set the test_size to 0.2. So, 80% data is for training and 20% of data is for testing. Test data is used to see how well does the model performs. \n",
    "The accuracy score for all classifiers is low. \n",
    "The accuracy score is calculated for test set. Logestic Regression has heighest accuracy of 69%. \n",
    "\n",
    "The accuracy score of k-NN classfier is 62%. It has the lowest accuracy.\n",
    "\n",
    "The accuracy score of Decision Tree classfier is 65%.\n",
    "\n",
    "The accuracy score of Gaussian Naive Bayes classfier is 63%.\n",
    "\n",
    "\n",
    "Accuracy score is not enough to justify the bias, as the predicted value might be because of the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuraqcy Score\n",
      "Accuracy Score on Test set KNeighborsClassifier   0.62\n",
      "Accuracy Score on Test set DecisionTreeClassifier 0.66\n",
      "Accuracy Score on Test set LogisticRegression     0.69\n",
      "Accuracy Score on Test set GaussianNB             0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = [kNN,DT,LR,m3]\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=0, test_size=1/3)\n",
    "print('Accuraqcy Score')\n",
    "for m in models:\n",
    "    mm = m.fit(X_train, y_train)\n",
    "    y_pred = mm.predict(X_test)\n",
    "    f1s = accuracy_score(y_test, y_pred) \n",
    "    print(\"Accuracy Score on Test set {:22} {:.2f}\".format(type(m).__name__, f1s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "In cross validation the data set is split into k folds and the each fold or group is used as the test set and the rest as training set. Here, the folds is set to 4, so this is 4-folds corss validation. \n",
    "\n",
    "\n",
    "The TN rate of k-NN classifier is 40% and it's TP rate is 77%, so this classifier is highly baised towards the majority class.\n",
    "\n",
    "The TN rate of Decision Tree Classfier is 57% and it's TP rate is 72%, so the classifier is baised towards the majority class.\n",
    "\n",
    "The TN rate of Logistic Regression Classifier is 43% and it's TP rate is 88%, so this classifier is highly baised towards the majority class.\n",
    "\n",
    "The TN rate of Gaussian Naive Bayes Classifier is 53% and it's TP rate is 73%, so this classifier is baised towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
    "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    "\n",
    "models = [kNN,DT,LR,m3]\n",
    "\n",
    "folds = 4\n",
    "v = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 x CV KNeighborsClassifier   FP: 0.60  TP: 0.77 TN: 0.40\n",
      "4 x CV DecisionTreeClassifier FP: 0.43  TP: 0.71 TN: 0.57\n",
      "4 x CV LogisticRegression     FP: 0.57  TP: 0.88 TN: 0.43\n",
      "4 x CV GaussianNB             FP: 0.47  TP: 0.73 TN: 0.53\n"
     ]
    }
   ],
   "source": [
    "TNP = []\n",
    "for m in models:\n",
    "    cv_results = cross_validate(m, X, y, cv= folds,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = v, n_jobs = -1)\n",
    "    fp_rate = cv_results['test_fp'].sum()/(cv_results['test_fp'].sum()+cv_results['test_tn'].sum())\n",
    "    tp_rate = cv_results['test_tp'].sum()/(cv_results['test_tp'].sum()+cv_results['test_fn'].sum())\n",
    "    tn_rate = cv_results['test_tn'].sum()/(cv_results['test_tn'].sum()+cv_results['test_fp'].sum())\n",
    "    TNP.append(round(tn_rate,2))\n",
    "  \n",
    "    print(\"{} x CV {:22} FP: {:.2f}  TP: {:.2f} TN: {:.2f}\".format(folds, type(m).__name__, fp_rate, tp_rate, tn_rate)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Predicted Score Plot\n",
    "The graph is plotted below.\n",
    "\n",
    "It can be observed that Logestic Regression has the low TN rate, 43%. This classifier is baised towards the majority class.\n",
    "\n",
    "Decision Tree Classifier has the TN rate of 60%. This classifer is also slightly baised towards the majority class.\n",
    "\n",
    "k-NN classifier has the TN rate of 40%. This classifer is baised towards the majority class.\n",
    "\n",
    "Gaussian Naive Bayes Classifier has the TN rate of 53%. This classifer is also baised towards the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.57, 0.43, 0.53]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAS+UlEQVR4nO3de7SdV13u8e9DaItUrJdujpi2pHDCGaMglBKKomC1rab0mHKp0loFFKwiEeRyNHipGESFWso4EIWA5Sa1YL1FiAaFYvHGyW4NbdNaG2I1mzpOU4uUUqBN+/OP9W5c7qy9s5LmXTvZ8/sZI4M15zvXu35ZI+VZ873MN1WFJKldD1nsAiRJi8sgkKTGGQSS1DiDQJIaZxBIUuMeutgF7K9jjz22VqxYsdhlSNJh5ZprrrmjqqZGbTvsgmDFihVMT08vdhmSdFhJ8i/zbfPQkCQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNe6wu7NYh6YV6z6y2CXsl1t/4+zFLkE6ZDgjkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapyrj0pqlqvmDjgjkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcb0GQZLVSW5OsiPJuhHbX5Rkd5Jt3Z+X9FmPJGlvvd1QlmQZsAE4E5gBtibZVFU3zhn6wapa21cdkqSF9TkjOBXYUVU7q+pe4ArgnB4/T5J0APoMguXArqH2TNc31/OSXJfkyiTHj9pRkguTTCeZ3r17dx+1SlKz+gyCjOirOe0/BVZU1ROBvwTeO2pHVbWxqlZV1aqpqamDXKYkta3PIJgBhn/hHwfcNjygqv69qr7SNd8JPKXHeiRJI/QZBFuBlUlOTHIkcB6waXhAkkcNNdcAN/VYjyRphN6uGqqqPUnWAluAZcBlVbU9yXpguqo2AS9PsgbYA9wJvKiveiRJo/X6PIKq2gxsntN30dDr1wKv7bMGSdLCvLNYkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmN6/Xh9ZIevBXrPrLYJeyXW3/j7MUuQfvJGYEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqXK9BkGR1kpuT7EiyboFx5yapJKv6rEeStLfegiDJMmADcBZwEnB+kpNGjHsE8HLgU33VIkmaX58zglOBHVW1s6ruBa4Azhkx7vXAm4Av91iLJGkefd5ZvBzYNdSeAZ42PCDJk4Hjq+rDSV4z346SXAhcCHDCCScccEHeoSlJe+tzRpARffXVjclDgEuBV+9rR1W1sapWVdWqqampg1iiJKnPIJgBjh9qHwfcNtR+BPAE4BNJbgW+DdjkCWNJmqw+g2ArsDLJiUmOBM4DNs1urKrPV9WxVbWiqlYAfw+sqarpHmuSJM3RWxBU1R5gLbAFuAn4UFVtT7I+yZq+PleStH96XYa6qjYDm+f0XTTP2NP6rEWSNJp3FktS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIat88gSHJ0t0AcSR6XZE2SI/ovTZI0CePMCK4GHpZkOfAx4EeB9/RZlCRpcsYJglTVPcBzgbdW1XMYPHFMkrQEjBUESb4duACYfbJLr2sUSZImZ5wg+BngtcAfdauHPga4qt+yJEmTss9f9lX1V8BfJTm6a+9k8LB5SdISMM5VQ9+e5EYGzxQgyZOS/FbvlUmSJmKcQ0NvAb4P+HeAqvo08Mw+i5IkTc5YN5RV1a45Xff3UIskaRGMc/XPriRPB6p79vDL6Q4TSZIOf+PMCH4SeBmwHJgBTu7akqQlYMEZQZJlwI9U1QUTqkeSNGELzgiq6n7gnAnVIklaBOOcI/ibJG8DPgh8cbazqq7trSpJ0sSMEwRP7/53/VBfAd9z8MuRJE3aOHcWf/ckCpEkLY5x7iw+Jsmbk0x3fy5JcswkipMk9W+cy0cvA74A/GD35y7g3X0WJUmanHHOETy2qp431P6VJNv6KkiSNFnjzAi+lOQ7ZxtJvgP4Un8lSZImaZwgeCmwIcmtSW4F3sbgbuN9SrI6yc1JdiRZN2L7Tya5Psm2JH+dxCefSdKEjXPV0DbgSUm+rmvfNc6Ou7uSNwBnMliaYmuSTVV149Cwy6vq7d34NcCbgdX791eQJD0Y41w19GtJvr6q7qqqu5J8Q5JfHWPfpwI7qmpnVd0LXMGcu5TnhMrRDO5PkCRN0DiHhs6qqv+YbVTV54BnjfG+5cDw8tUzXd9/k+RlST4DvIl5nnyW5MLZy1d37949xkdLksY1ThAsS3LUbCPJ1wBHLTD+q0NH9O31i7+qNlTVY4GfA35x1I6qamNVraqqVVNTU2N8tCRpXONcPvq7wMeSvJvB/5H/GPDeMd43Axw/1D4OuG2B8VcAvz3GfiVJB9E4J4vflOQ64Iyu6/VVtWWMfW8FViY5EfgscB7wQ8MDkqysqlu65tnALUiSJmqcGQFV9edJtjJ4VvEdY75nT5K1wBZgGXBZVW1Psh6YrqpNwNokZwD3AZ8DXnggfwlJ0oGbNwiSfBhYV1U3JHkUcC0wDTw2ycaqesu+dl5Vm4HNc/ouGnr9igOuXJJ0UCx0svjEqrqhe/2jwF9U1fcDT2NwnkCStAQsFAT3Db0+ne6XfVV9AXigz6IkSZOz0DmCXUl+msHVP6cAfw5fvXz0iAnUJkmagIVmBC8GHg+8CHj+0E1l34bLUEvSkjHvjKCqbmfE4nJVdRVwVZ9FSZImZ5w7iyVJS5hBIEmNMwgkqXEL3VB20XzbgKqq1/dQjyRpwha6fPSLI/oeDrwE+CbAIJCkJWChq4YumX2d5BHAKxjcUXwFcMl875MkHV4WXHQuyTcCrwIuYLD09Cndg2kkSUvEQucILgaeC2wEvrWq7p5YVZKkiVnoqqFXA9/C4KlhtyW5q/vzhSRjPcBeknToW+jQ0Ker6skTq0SStCgWmhHs9XxhSdLSs9CM4JFJXjXfxqp6cw/1SJImbKEgWAZ8LZAJ1SJJWgQLBcG/VdX6iVUiSVoUC50jcCYgSQ1YKAhOn1gVkqRFM28QVNWdkyxEkrQ4XIZakhpnEEhS4wwCSWqcQSBJjTMIJKlxvQZBktVJbk6yI8m6EdtfleTGJNcl+ViSR/dZjyRpb70FQZJlwAbgLOAk4PwkJ80Z9g/Aqqp6InAl8Ka+6pEkjdbnjOBUYEdV7ayqexk84vKc4QFVdVVV3dM1/x44rsd6JEkj9BkEy4FdQ+2Zrm8+Lwb+bNSGJBcmmU4yvXv37oNYoiSpzyAYtVbRyGccJPlhYBVw8ajtVbWxqlZV1aqpqamDWKIkacGH1z9IM8DxQ+3jgNvmDkpyBvALwHdV1Vd6rEeSNEKfM4KtwMokJyY5EjgP2DQ8IMmTgXcAa6rq9h5rkSTNo7cgqKo9wFpgC3AT8KGq2p5kfZI13bCLGTz85veTbEuyaZ7dSZJ60uehIapqM7B5Tt9FQ6/P6PPzJUn75p3FktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjeg2CJKuT3JxkR5J1I7Y/M8m1SfYkObfPWiRJo/UWBEmWARuAs4CTgPOTnDRn2L8CLwIu76sOSdLCHtrjvk8FdlTVToAkVwDnADfODqiqW7ttD/RYhyRpAX0eGloO7Bpqz3R9+y3JhUmmk0zv3r37oBQnSRroMwgyoq8OZEdVtbGqVlXVqqmpqQdZliRpWJ9BMAMcP9Q+Dritx8+TJB2APoNgK7AyyYlJjgTOAzb1+HmSpAPQWxBU1R5gLbAFuAn4UFVtT7I+yRqAJE9NMgP8APCOJNv7qkeSNFqfVw1RVZuBzXP6Lhp6vZXBISNJ0iLxzmJJapxBIEmNMwgkqXEGgSQ1ziCQpMYZBJLUOINAkhpnEEhS4wwCSWqcQSBJjTMIJKlxBoEkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY0zCCSpcQaBJDXOIJCkxhkEktQ4g0CSGmcQSFLjDAJJapxBIEmNMwgkqXEGgSQ1rtcgSLI6yc1JdiRZN2L7UUk+2G3/VJIVfdYjSdpbb0GQZBmwATgLOAk4P8lJc4a9GPhcVf1P4FLgjX3VI0karc8ZwanAjqraWVX3AlcA58wZcw7w3u71lcDpSdJjTZKkOR7a476XA7uG2jPA0+YbU1V7knwe+CbgjuFBSS4ELuyadye5uZeKD9yxzKn5YIjzI/C77ZPfbX8Oxe/20fNt6DMIRv2yrwMYQ1VtBDYejKL6kGS6qlYtdh1Lkd9tf/xu+3O4fbd9HhqaAY4fah8H3DbfmCQPBY4B7uyxJknSHH0GwVZgZZITkxwJnAdsmjNmE/DC7vW5wMeraq8ZgSSpP70dGuqO+a8FtgDLgMuqanuS9cB0VW0Cfgd4f5IdDGYC5/VVT88O2cNWS4DfbX/8bvtzWH238Qe4JLXNO4slqXEGgSQ1ziBYQJK7h14/K8ktSU5I8rok9yR55DxjK8klQ+3XJHndxAo/DCW5P8m2JNuTfDrJq5I8JMn3df3bktzdLVmyLcn7Frvmw8nwv8+hvtcl+Wz3fd6Y5PzFqO1wkuR/JLk8yc4k1yT5uyTPSXJa99/99w+N/XCS07rXnxj6t3tTd2/UIcMgGEOS04G3Aqur6l+77juAV8/zlq8Az01y7CTqWyK+VFUnV9XjgTOBZwG/XFVbuv6TgWnggq79gkWtdum4tPtuzwHekeSIxS7oUNWtevDHwNVV9ZiqegqDC1yO64bMAL+wwC4u6L7r7wDe2F1NeUgwCPYhyTOAdwJnV9VnhjZdBjw/yTeOeNseBlcNvHICJS45VXU7gzvJ17rkyGRU1S3APcA3LHYth7DvAe6tqrfPdlTVv1TVW7vmp4HPJzlzH/v5WuCLwP39lLn/DIKFHQX8CfDsqvrHOdvuZhAGr5jnvRuAC5Ic02N9S1ZV7WTw7/OR+xqrBy/JKcAtXQhrtMcD1+5jzK8CvzjPtg8kuQ64GXh9VRkEh4n7gL9lsErqKP8XeGGSr5u7oaruAt4HvLy/8pY8ZwP9e2W3dtengNctci2HlSQbuvNZW2f7quqT3bZnjHjLBVX1ROAE4DVJ5l37Z9IMgoU9APwg8NQkPz93Y1X9B3A58FPzvP8tDELk6N4qXKKSPIbB1NlfqP26tKr+F/B84H1JHrbYBR3CtgOnzDaq6mXA6cDUnHFvYIFzBVW1m8HMYu4inIvGINiHqroH+N8MDvOMmhm8GfgJRtylXVV3Ah9i/hmFRkgyBbwdeJtLjkxGVf0hg5PxL9zX2IZ9HHhYkpcO9T187qCq+iiDcy1PGrWTJA8Hngx8ZtT2xdDn6qNLRlXdmWQ1cHWSO+ZsuyPJHzH/ieFLgLV917gEfE2SbcARDE62v59ByOrgeHiSmaH2qO92PXB5kndW1QMTquuwUVWV5NnApUl+FtjN4KTvz40Y/gYG5xeHfSDJlxice3xPVV3Ta8H7wSUmJKlxHhqSpMYZBJLUOINAkhpnEEhS4wwCSWqcQaBmJPnmJFck+Uy32ubmJI9LcsNB/Iz1Sc7oXj+jW011W5LlSa48WJ8jHUxePqomdIvX/S3w3tlFw5KcDDwC+O2qekIPn/l24FNV9e4DeO+yQ2ktGi1tzgjUiu8G7puzcuQ2YNdsO8mKJJ9Mcm335+ld/6OSXN39sr+h+6W/LMl7uvb1SV7ZjX1PknOTvITB8iQXJflAt+8bujHLklycZGuS65L8RNd/WpKrklwOXJ/k6CQf6dazuSHJ8yf2bakp3lmsVjwB2NednLcDZ1bVl5OsBH4PWAX8ELClqt6QZBmDZQVOBpbPziSSfP3wjqrqXUm+E/hwVV2ZZMXQ5hcDn6+qpyY5CvibJB/ttp0KPKGq/jnJ84Dbqurs7jNcyVa9MAik/3IE8LbukNH9wOO6/q3AZd1DW/64qrYl2Qk8JslbgY8AHx25x9G+F3hiknO79jHASuBe4P9V1T93/dcDv5nkjQwC5ZMP5i8nzcdDQ2rFduAp+xjzSuD/M1gsbBVwJEBVXQ08E/gs8P4kL6iqz3XjPgG8DHjXftQS4Kdnn7xWVSd2C5XBYO0aus/9p67m64FfT3LRfnyGNDaDQK34OHBUkh+f7UjyVGB4TfhjgH/rFlz7EWBZN+7RwO1V9U7gd4BTuseQPqSq/gD4JYaWJx7DFuCls4+F7K5c2mup8iTfAtxTVb8L/OZ+foY0Ng8NqQndypHPAd6SZB3wZeBW4GeGhv0W8AdJfgC4iv/6dX4a8H+S3MfgyXQvAJYD704y+2PqtftRzruAFcC13dVMu4Fnjxj3rcDFSR5g8JCkl44YIz1oXj4qSY3z0JAkNc4gkKTGGQSS1DiDQJIaZxBIUuMMAklqnEEgSY37T5kb4/FQtq8yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "Yplot = TNP\n",
    "print(Yplot)\n",
    "plt.bar(['KNN', 'DT', 'LR', 'GNB'], Yplot, 0.5)\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('TN Scores')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sampling\n",
    "Random over sampler is used to overcome the bias. This generates artificial data and overcomes the bias. The random state is set to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=2)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After oversampling, more samples of the minority class is produced. We used random oversampler. reviewNotHelpfulness is at 47%.\n",
    "\n",
    "The predicted score of k-NN classifier is 55%. It is now biased towards minority class.\n",
    "\n",
    "The predicted score of Decision Tree classifier is 51%. It is now biased towards minority class.\n",
    "\n",
    "The predicted score of Logistic Regression classifier is 44%. It is now slighly biased towards majority class.\n",
    "\n",
    "The predicted score of Gaussian Naive Bayes Regression classifier is 44%. It is now slighly biased towards majority class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier\n",
      "(462, 23) (154, 23)\n",
      "Review is not helpfull: 0.47\n",
      "Review is helpfull : 0.55\n",
      "DecisionTreeClassifier\n",
      "(462, 23) (154, 23)\n",
      "Review is not helpfull: 0.47\n",
      "Review is helpfull : 0.55\n",
      "LogisticRegression\n",
      "(462, 23) (154, 23)\n",
      "Review is not helpfull: 0.47\n",
      "Review is helpfull : 0.44\n",
      "GaussianNB\n",
      "(462, 23) (154, 23)\n",
      "Review is not helpfull: 0.47\n",
      "Review is helpfull : 0.44\n"
     ]
    }
   ],
   "source": [
    "models = [kNN,DT,LR,m3]\n",
    "for m in models:\n",
    "    print(type(m).__name__)\n",
    "\n",
    "    y_pred = m.fit(X_train, y_train).predict(X_test)\n",
    "    print(X_train.shape,X_test.shape)\n",
    "    y_test.sum()/len(y_test)\n",
    "    print(\"Review is not helpfull: %0.2f\" % (1- (y_test.sum()/len(y_test))))\n",
    "    print(\"Review is helpfull : %0.2f\" % (1- (y_pred.sum()/len(y_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Out Testing\n",
    "The overall accuracy of all the classifiers improved. \n",
    "\n",
    "Logistic regression Classifier has 75% accuracy. \n",
    "\n",
    "Decision tree Classifier has 79% accuracy. This classifier has the highest accuracy.\n",
    "\n",
    "Gaussian Naive Bayes Classifier has 68% accuracy.\n",
    "\n",
    "k-NN Classifier has 70% accuracy.\n",
    "\n",
    "\n",
    "The accuracy scores of all classifiers improved after oversampling. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score\n",
      "F1 Score on Test set KNeighborsClassifier   0.70\n",
      "F1 Score on Test set DecisionTreeClassifier 0.79\n",
      "F1 Score on Test set LogisticRegression     0.75\n",
      "F1 Score on Test set GaussianNB             0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = [kNN,DT,LR,m3]\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, random_state=0, test_size=1/3)\n",
    "print('Accuracy Score')\n",
    "for m in models:\n",
    "    mm = m.fit(X_train, y_train)\n",
    "    y_pred1 = mm.predict(X_test)\n",
    "    f1s = f1_score(y_test, y_pred1) \n",
    "    print(\"F1 Score on Test set {:22} {:.2f}\".format(type(m).__name__, f1s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "I performed 5-fold corss validation. \n",
    "\n",
    "The TN rate for logistic regression classifier is 69% and the Tp rate is 68%, so it is not biased. \n",
    "\n",
    "The TN rate for k-NN classifier is 76% and the Tp rate is 64%, so it is biased towards minority class.\n",
    "\n",
    "The TN rate for Decision Tree classifier is 89% and the Tp rate is 70%, so it is highly biased towards minority class.\n",
    "\n",
    "The TN rate for Gaussian Naive Bayes classifier is 65% and the Tp rate is 61%, so it is slightly biased towards minority class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "scoring = {'tp' : make_scorer(tp), 'tn' : make_scorer(tn),\n",
    "           'fp' : make_scorer(fp), 'fn' : make_scorer(fn)}\n",
    "\n",
    "models = [kNN,DT,LR,m3]\n",
    "\n",
    "folds = 5\n",
    "v = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 x CV KNeighborsClassifier   FP: 0.24  TP: 0.64 TN: 0.76\n",
      "5 x CV DecisionTreeClassifier FP: 0.12  TP: 0.70 TN: 0.88\n",
      "5 x CV LogisticRegression     FP: 0.31  TP: 0.69 TN: 0.69\n",
      "5 x CV GaussianNB             FP: 0.35  TP: 0.61 TN: 0.65\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    cv_results = cross_validate(m, X_resampled, y_resampled, cv= folds,scoring=scoring, return_train_score=False, \n",
    "                                    verbose = v, n_jobs = -1)\n",
    "    fp_rate = cv_results['test_fp'].sum()/(cv_results['test_fp'].sum()+cv_results['test_tn'].sum())\n",
    "    tp_rate = cv_results['test_tp'].sum()/(cv_results['test_tp'].sum()+cv_results['test_fn'].sum())\n",
    "    tn_rate = cv_results['test_tn'].sum()/(cv_results['test_tn'].sum()+cv_results['test_fp'].sum())\n",
    "  \n",
    "    print(\"{} x CV {:22} FP: {:.2f}  TP: {:.2f} TN: {:.2f}\".format(folds, type(m).__name__, fp_rate, tp_rate, tn_rate)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
